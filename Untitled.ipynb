{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Dense , Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import EarlyStopping , LearningRateScheduler\n",
    "import keras.backend as K\n",
    "from keras.initializers import RandomUniform\n",
    "from keras import metrics , regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6232, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef splitData(X,rate):\\n    train = X[:int(X.shape[0]*rate)]\\n    val = X[int(X.shape[0]*rate):]\\n    return train, val\\n\\ndef periods(data,days,data_date=None):\\n    y = []\\n    x = []\\n    p_dates = []\\n    for i in range(0,len(data),days):\\n        if days*(i+1)>=len(data)-1:\\n            break\\n        x.append(data[i:i+days])\\n        if type(data_date) != type(None):\\n            p_dates.append(data_date[i:i+days])\\n        if data[days*(i+1)][0] > x[-1][-1][0]:\\n            input()\\n            print(data[days*(i+1)][0],x[-1][-1][0])\\n            y.append(1)\\n        else:\\n            y.append(0)\\n    return np.array(x),np.array(y),p_dates\\ndef unperiods(data,days):\\n    l = []\\n    for a in data:\\n        l.extend(a)\\n    return l\\ndf = pd.read_csv(\"data/2330_indicators.csv\").drop(range(11))\\nprint(np.array(df.values).shape)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitData(X,rate):\n",
    "    train = X[:int(X.shape[0]*rate)]\n",
    "    val = X[int(X.shape[0]*rate):]\n",
    "    return train, val\n",
    "\n",
    "def periods(data,days,data_date=None):\n",
    "    y = []\n",
    "    x = []\n",
    "    p_dates = []\n",
    "    for i in range(0,len(data),days):\n",
    "        if i+days>=len(data)-1:\n",
    "            break\n",
    "        x.append(data[i:i+days])\n",
    "        if type(data_date) != type(None):\n",
    "            p_dates.append(data_date[i:i+days])\n",
    "        if data[i+days][0] > data[i][0]:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return np.array(x),np.array(y),p_dates\n",
    "def unperiods(data,days):\n",
    "    l = []\n",
    "    for a in data:\n",
    "        l.extend(a)\n",
    "    return l\n",
    "df = pd.read_csv(\"data/2330_indicators.csv\").drop(range(11))\n",
    "print(np.array(df.values).shape)\n",
    "\"\"\"\n",
    "def splitData(X,rate):\n",
    "    train = X[:int(X.shape[0]*rate)]\n",
    "    val = X[int(X.shape[0]*rate):]\n",
    "    return train, val\n",
    "\n",
    "def periods(data,days,data_date=None):\n",
    "    y = []\n",
    "    x = []\n",
    "    p_dates = []\n",
    "    for i in range(0,len(data),days):\n",
    "        if days*(i+1)>=len(data)-1:\n",
    "            break\n",
    "        x.append(data[i:i+days])\n",
    "        if type(data_date) != type(None):\n",
    "            p_dates.append(data_date[i:i+days])\n",
    "        if data[days*(i+1)][0] > x[-1][-1][0]:\n",
    "            input()\n",
    "            print(data[days*(i+1)][0],x[-1][-1][0])\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "    return np.array(x),np.array(y),p_dates\n",
    "def unperiods(data,days):\n",
    "    l = []\n",
    "    for a in data:\n",
    "        l.extend(a)\n",
    "    return l\n",
    "df = pd.read_csv(\"data/2330_indicators.csv\").drop(range(11))\n",
    "print(np.array(df.values).shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311, 20, 9)\n"
     ]
    }
   ],
   "source": [
    "p_days = 20\n",
    "sc = MinMaxScaler()\n",
    "#移除前11筆因累積計算技術指標為0\n",
    "data_sc = sc.fit_transform(df.drop(['date'],axis=1).values)\n",
    "x,y,p_dates = periods(data_sc,p_days,df.iloc[:,[0]].values)\n",
    "print(x.shape)\n",
    "X_train , X_val = splitData(x,0.8)\n",
    "Y_train , Y_val = splitData(y,0.8)\n",
    "xx,pd = splitData(np.array(p_dates),0.8)\n",
    "p_date = unperiods(pd,p_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20, 32)            5376      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,729\n",
      "Trainable params: 13,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 380\n",
    "batch_size = 16\n",
    "LSTM_num = 3\n",
    "units = 32\n",
    "\n",
    "model_lstm = Sequential()\n",
    "#use L2 to improve overfit\n",
    "model_lstm.add(LSTM(units,input_shape=X_train.shape[1:],activation='relu',kernel_initializer='random_uniform',return_sequences=True)) \n",
    "#model_lstm.add(Dropout(0.001))\n",
    "\n",
    "#model_lstm.add(LSTM(units,activation='relu',return_sequences=True))\n",
    "#model_lstm.add(Dropout(0.001))\n",
    "\n",
    "model_lstm.add(LSTM(units,activation='relu',return_sequences=False))\n",
    "#model_lstm.add(Dropout(0.001))\n",
    "\n",
    "model_lstm.add(Dense(1,activation='sigmoid'))\n",
    "model_lstm.summary()\n",
    "model_lstm.compile(loss='binary_crossentropy',optimizer=\"Adam\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 248 samples, validate on 63 samples\n",
      "Epoch 1/380\n",
      "248/248 [==============================] - 4s 17ms/step - loss: 0.6878 - acc: 0.5282 - val_loss: 0.6791 - val_acc: 0.5873\n",
      "Epoch 2/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.6751 - acc: 0.5282 - val_loss: 0.6481 - val_acc: 0.5873\n",
      "Epoch 3/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.6441 - acc: 0.5565 - val_loss: 0.5569 - val_acc: 0.6667\n",
      "Epoch 4/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.5935 - acc: 0.7379 - val_loss: 0.4911 - val_acc: 0.7302\n",
      "Epoch 5/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.5674 - acc: 0.7460 - val_loss: 0.5689 - val_acc: 0.7619\n",
      "Epoch 6/380\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 0.5604 - acc: 0.7581 - val_loss: 0.6400 - val_acc: 0.7302\n",
      "Epoch 7/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.5433 - acc: 0.7298 - val_loss: 0.4711 - val_acc: 0.7143\n",
      "Epoch 8/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.5043 - acc: 0.7661 - val_loss: 0.4644 - val_acc: 0.6984\n",
      "Epoch 9/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4826 - acc: 0.7661 - val_loss: 0.4755 - val_acc: 0.7143\n",
      "Epoch 10/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4729 - acc: 0.7903 - val_loss: 0.4922 - val_acc: 0.7143\n",
      "Epoch 11/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4461 - acc: 0.7984 - val_loss: 0.4631 - val_acc: 0.7143\n",
      "Epoch 12/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4413 - acc: 0.7702 - val_loss: 0.4335 - val_acc: 0.6984\n",
      "Epoch 13/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4252 - acc: 0.7863 - val_loss: 0.4754 - val_acc: 0.7460\n",
      "Epoch 14/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4243 - acc: 0.7944 - val_loss: 0.4472 - val_acc: 0.6984\n",
      "Epoch 15/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4432 - acc: 0.7661 - val_loss: 0.4621 - val_acc: 0.7143\n",
      "Epoch 16/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4803 - acc: 0.7258 - val_loss: 0.5015 - val_acc: 0.7619\n",
      "Epoch 17/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4296 - acc: 0.8024 - val_loss: 0.4579 - val_acc: 0.6984\n",
      "Epoch 18/380\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 0.4215 - acc: 0.7742 - val_loss: 0.4319 - val_acc: 0.6984\n",
      "Epoch 19/380\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 0.4401 - acc: 0.7661 - val_loss: 0.4467 - val_acc: 0.7302\n",
      "Epoch 20/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4057 - acc: 0.8024 - val_loss: 0.4834 - val_acc: 0.7460\n",
      "Epoch 21/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4119 - acc: 0.7581 - val_loss: 0.4397 - val_acc: 0.6984\n",
      "Epoch 22/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4078 - acc: 0.7823 - val_loss: 0.4492 - val_acc: 0.7143\n",
      "Epoch 23/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4272 - acc: 0.7863 - val_loss: 0.4495 - val_acc: 0.7460\n",
      "Epoch 24/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4003 - acc: 0.8065 - val_loss: 0.5105 - val_acc: 0.7460\n",
      "Epoch 25/380\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 0.3972 - acc: 0.7984 - val_loss: 0.4885 - val_acc: 0.7143\n",
      "Epoch 26/380\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 0.3811 - acc: 0.8024 - val_loss: 0.5246 - val_acc: 0.7302\n",
      "Epoch 27/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3892 - acc: 0.7863 - val_loss: 0.4724 - val_acc: 0.7143\n",
      "Epoch 28/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3882 - acc: 0.7903 - val_loss: 0.4990 - val_acc: 0.7143\n",
      "Epoch 29/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3972 - acc: 0.7984 - val_loss: 0.5088 - val_acc: 0.6984\n",
      "Epoch 30/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3863 - acc: 0.7903 - val_loss: 0.5599 - val_acc: 0.7302\n",
      "Epoch 31/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3886 - acc: 0.7984 - val_loss: 0.6613 - val_acc: 0.7143\n",
      "Epoch 32/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3753 - acc: 0.7944 - val_loss: 0.6757 - val_acc: 0.7143\n",
      "Epoch 33/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3785 - acc: 0.8065 - val_loss: 0.9902 - val_acc: 0.7143\n",
      "Epoch 34/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3876 - acc: 0.8065 - val_loss: 1.4605 - val_acc: 0.6984\n",
      "Epoch 35/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3919 - acc: 0.7823 - val_loss: 0.5009 - val_acc: 0.7619\n",
      "Epoch 36/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4304 - acc: 0.7903 - val_loss: 0.4656 - val_acc: 0.7778\n",
      "Epoch 37/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.3988 - acc: 0.8266 - val_loss: 0.7105 - val_acc: 0.7143\n",
      "Epoch 38/380\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 0.4259 - acc: 0.8105 - val_loss: 0.4575 - val_acc: 0.7778\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "#decay lr by half every 10 epoch\n",
    "#lrate = LearningRateScheduler(step_decay)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "history = model_lstm.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, shuffle=False,validation_data=(X_val,Y_val),callbacks=[early_stop])\n",
    "y_pred_test_lstm = model_lstm.predict(X_val)\n",
    "y_train_pred_lstm = model_lstm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7155388518981171\n"
     ]
    }
   ],
   "source": [
    "y_preds= model_lstm.predict(X_val)\n",
    "print(np.mean(history.history['val_acc']))\n",
    "#accuracy when the predictions which is bigger than threshold and equal to y\n",
    "def acc_threshold(preds,rate):\n",
    "    re = []\n",
    "    for i,y in enumerate(preds):\n",
    "        if y > rate:\n",
    "            y=1\n",
    "        else:\n",
    "            y=0\n",
    "        if Y_val[i]==y:\n",
    "            re.append(1)\n",
    "        else:\n",
    "            re.append(0)\n",
    "    return np.mean(re)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = []\n",
    "for i in np.arange(0.5,1,0.05):\n",
    "    thresholds.append(np.round(acc_threshold(y_preds,i),decimals=2))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(0.5,1,0.05),thresholds,'b',label='Thresholds')\n",
    "plt.title('Accuracy by threshold',)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Rate')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Rate Of Return\n",
    "x: trended data\n",
    "y: predictions\n",
    "r: trade if y > r\n",
    "p: period\n",
    "s: short if y < r\n",
    "\"\"\"\n",
    "def ROR(x,y,r,p,init_funds,s=False):\n",
    "    dict_Info = {\n",
    "        'total_costs':[],\n",
    "        'total_prices':[],\n",
    "        'trade_times':[],\n",
    "        'date_msg':[],\n",
    "        \"RORs\":[]\n",
    "    }\n",
    "    funds = init_funds\n",
    "    profit=0\n",
    "    total_cost=0\n",
    "    total_price=0\n",
    "    acc = []\n",
    "    times = 0\n",
    "    d_msg = ''\n",
    "    for i,pre in enumerate(y[:-1]):\n",
    "        price = x[(i+1)*p_days][0]\n",
    "        cost = x[i*p_days][0]\n",
    "        quantity = int(funds/cost)\n",
    "        if pre>r:\n",
    "            times+=1\n",
    "            total_cost+= cost * quantity\n",
    "            total_price+= price  * quantity\n",
    "            funds+= (price-cost)*quantity            \n",
    "            d_msg += \"\\nBuy Date: {} - {}\\nSell Date: {} - {}\\nQuantity: {}\\n\".format(p_date[i*p_days],cost,p_date[(i+1)*p_days],price,quantity)\n",
    "            if price > cost:\n",
    "                acc.append(1)\n",
    "            else:\n",
    "                acc.append(0)\n",
    "        elif s:\n",
    "            times+=1\n",
    "            total_cost+= price * quantity\n",
    "            total_price+= cost * quantity\n",
    "            funds+= (cost-price) * quantity\n",
    "            pass\n",
    "        if (i+1)%p==0:\n",
    "            #pass and not record the rate of prediciotn 'pre' < rate of threshold 'r'\n",
    "            if total_cost != 0:\n",
    "                dict_Info['total_costs'].append(total_cost*1000)\n",
    "                dict_Info['total_prices'].append(total_price*1000)\n",
    "                dict_Info['trade_times'].append(times)\n",
    "                dict_Info['date_msg'].append(d_msg)\n",
    "                dict_Info['RORs'].append((funds)/init_funds)\n",
    "            times=0\n",
    "            funds = init_funds\n",
    "            total_cost=0\n",
    "            total_price=0\n",
    "            d_msg = ''\n",
    "    #in case that dont have enough tra\n",
    "    if len(dict_Info['RORs']) ==0:\n",
    "        dict_Info['total_costs'].append(total_cost*1000)\n",
    "        dict_Info['total_prices'].append(total_price*1000)\n",
    "        dict_Info['trade_times'].append(times)\n",
    "        dict_Info['date_msg'].append(d_msg)\n",
    "        dict_Info['RORs'].append((funds)/init_funds)\n",
    "    print(\"Trend Accuracy:\",np.mean(acc))\n",
    "    return dict_Info,init_funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Accuracy: 0.775\n",
      "\n",
      "Buy Date: ['102/12/09'] - 105.0\n",
      "Sell Date: ['103/01/07'] - 102.0\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/01/07'] - 102.0\n",
      "Sell Date: ['103/02/12'] - 105.0\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/02/12'] - 105.0\n",
      "Sell Date: ['103/03/13'] - 116.5\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/03/13'] - 116.5\n",
      "Sell Date: ['103/04/11'] - 120.0\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/05/12'] - 118.50000000000001\n",
      "Sell Date: ['103/06/10'] - 124.5\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/06/10'] - 124.5\n",
      "Sell Date: ['103/07/08'] - 134.5\n",
      "Quantity: 9\n",
      "\n",
      "Buy Date: ['103/08/06'] - 120.49999999999999\n",
      "Sell Date: ['103/09/03'] - 128.0\n",
      "Quantity: 10\n",
      "\n",
      "Buy Date: ['103/10/02'] - 121.0\n",
      "Sell Date: ['103/10/31'] - 130.5\n",
      "Quantity: 11\n",
      "\n",
      "Buy Date: ['103/10/31'] - 130.5\n",
      "Sell Date: ['103/11/28'] - 141.5\n",
      "Quantity: 11\n",
      "\n",
      "Total Costs: 10015000.0\n",
      "Trade Times: 9\n",
      "Initial Funds: 1000000\n",
      "Profits: 579500.0\n",
      "Return Of Rate: 1.5795\n",
      "\n",
      "\n",
      "Buy Date: ['103/12/26'] - 139.0\n",
      "Sell Date: ['104/01/26'] - 145.0\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['104/01/26'] - 145.0\n",
      "Sell Date: ['104/03/04'] - 150.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['104/03/04'] - 150.5\n",
      "Sell Date: ['104/04/01'] - 144.0\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['104/04/01'] - 144.0\n",
      "Sell Date: ['104/05/04'] - 147.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['104/06/01'] - 145.0\n",
      "Sell Date: ['104/06/30'] - 140.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['104/08/26'] - 122.5\n",
      "Sell Date: ['104/09/23'] - 124.5\n",
      "Quantity: 8\n",
      "\n",
      "Buy Date: ['104/09/23'] - 124.5\n",
      "Sell Date: ['104/10/26'] - 140.0\n",
      "Quantity: 8\n",
      "\n",
      "Total Costs: 7040500.0\n",
      "Trade Times: 7\n",
      "Initial Funds: 1000000\n",
      "Profits: 168000.0\n",
      "Return Of Rate: 1.168\n",
      "\n",
      "\n",
      "Buy Date: ['104/11/23'] - 141.0\n",
      "Sell Date: ['104/12/21'] - 141.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/01/19'] - 138.0\n",
      "Sell Date: ['105/02/24'] - 148.0\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/02/24'] - 148.0\n",
      "Sell Date: ['105/03/24'] - 158.49999999999997\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/03/24'] - 158.49999999999997\n",
      "Sell Date: ['105/04/25'] - 157.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/05/24'] - 151.0\n",
      "Sell Date: ['105/06/22'] - 166.0\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/06/22'] - 166.0\n",
      "Sell Date: ['105/07/21'] - 171.99999999999997\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/07/21'] - 171.99999999999997\n",
      "Sell Date: ['105/08/18'] - 175.99999999999997\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/08/18'] - 175.99999999999997\n",
      "Sell Date: ['105/09/14'] - 173.5\n",
      "Quantity: 7\n",
      "\n",
      "Buy Date: ['105/09/14'] - 173.5\n",
      "Sell Date: ['105/10/19'] - 189.49999999999997\n",
      "Quantity: 7\n",
      "\n",
      "Total Costs: 9968000.0\n",
      "Trade Times: 9\n",
      "Initial Funds: 1000000\n",
      "Profits: 409500.0\n",
      "Return Of Rate: 1.4094999999999998\n",
      "\n",
      "\n",
      "Buy Date: ['105/11/16'] - 180.0\n",
      "Sell Date: ['105/12/14'] - 187.49999999999997\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['105/12/14'] - 187.49999999999997\n",
      "Sell Date: ['106/01/12'] - 184.5\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/01/12'] - 184.5\n",
      "Sell Date: ['106/02/17'] - 189.49999999999997\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/04/19'] - 186.5\n",
      "Sell Date: ['106/05/18'] - 203.49999999999997\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/05/18'] - 203.49999999999997\n",
      "Sell Date: ['106/06/16'] - 211.5\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/07/14'] - 213.0\n",
      "Sell Date: ['106/08/11'] - 212.49999999999997\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/08/11'] - 212.49999999999997\n",
      "Sell Date: ['106/09/08'] - 218.0\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/09/08'] - 218.0\n",
      "Sell Date: ['106/10/06'] - 224.5\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['106/10/06'] - 224.5\n",
      "Sell Date: ['106/11/07'] - 244.00000000000003\n",
      "Quantity: 5\n",
      "\n",
      "Total Costs: 9050000.0\n",
      "Trade Times: 9\n",
      "Initial Funds: 1000000\n",
      "Profits: 327500.0\n",
      "Return Of Rate: 1.3275\n",
      "\n",
      "\n",
      "Buy Date: ['106/12/05'] - 229.5\n",
      "Sell Date: ['107/01/03'] - 237.00000000000003\n",
      "Quantity: 4\n",
      "\n",
      "Buy Date: ['107/01/03'] - 237.00000000000003\n",
      "Sell Date: ['107/01/31'] - 255.00000000000003\n",
      "Quantity: 4\n",
      "\n",
      "Buy Date: ['107/01/31'] - 255.00000000000003\n",
      "Sell Date: ['107/03/09'] - 250.50000000000003\n",
      "Quantity: 4\n",
      "\n",
      "Buy Date: ['107/07/05'] - 214.49999999999997\n",
      "Sell Date: ['107/08/02'] - 244.5\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['107/08/02'] - 244.5\n",
      "Sell Date: ['107/08/30'] - 263.5\n",
      "Quantity: 5\n",
      "\n",
      "Buy Date: ['107/08/30'] - 263.5\n",
      "Sell Date: ['107/09/28'] - 262.5\n",
      "Quantity: 5\n",
      "\n",
      "Total Costs: 6498500.0\n",
      "Trade Times: 6\n",
      "Initial Funds: 1000000\n",
      "Profits: 324000.0\n",
      "Return Of Rate: 1.3240000000000003\n",
      "\n",
      "Average of Years ROR: 1.362\n"
     ]
    }
   ],
   "source": [
    "d,funds = ROR(sc.inverse_transform(unperiods(X_val,p_days)),y_pred_test_lstm,0.5,12,1000)\n",
    "\n",
    "for i in range(len(d['RORs'])):\n",
    "    msg = '{}\\nTotal Costs: {}\\nTrade Times: {}\\nInitial Funds: {}\\nProfits: {}\\nReturn Of Rate: {}\\n'.format(d['date_msg'][i],\n",
    "                                                                                    d['total_costs'][i],\n",
    "                                                                                     d['trade_times'][i],\n",
    "                                                                                    funds*1000,\n",
    "                                                                                     d['total_prices'][i]-d['total_costs'][i],\n",
    "                                                                                     d['RORs'][i])\n",
    "    print(msg)\n",
    "print(\"Average of Years ROR: {:.3f}\".format(np.mean(d['RORs'])))\n",
    "#print(\"Total cost: {:.2f}\\nProfits:{:.2f}\\nReturn of Rate:{:.2f}\".format(t,p,p/t*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
