{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import LSTM, Dense, Dropout, Input, Concatenate\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics, regularizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.02900000e+03 1.04500000e+02 2.41352170e+07 ... 6.14545459e+01\n",
      "  7.47828249e+01 5.72801873e+01]\n",
      " [5.03000000e+03 1.06000000e+02 4.42800980e+07 ... 7.06053325e+01\n",
      "  8.31885499e+01 6.59163081e+01]\n",
      " [5.03100000e+03 1.05500000e+02 2.44846530e+07 ... 6.44820527e+01\n",
      "  8.50886629e+01 7.23070931e+01]\n",
      " ...\n",
      " [6.28400000e+03 2.29000000e+02 2.86349250e+07 ... 6.28794573e+01\n",
      "  7.73671028e+01 7.51518098e+01]\n",
      " [6.28500000e+03 2.27000000e+02 2.36477510e+07 ... 5.55777020e+01\n",
      "  7.10225130e+01 7.37753776e+01]\n",
      " [6.28600000e+03 2.27000000e+02 2.23697990e+07 ... 5.55777020e+01\n",
      "  6.67927864e+01 7.14478472e+01]] (1258, 10)\n"
     ]
    }
   ],
   "source": [
    "def splitData(X,Y,rate):\n",
    "    X_train = X[:int(X.shape[0]*rate)]\n",
    "    Y_train = Y[:int(Y.shape[0]*rate)]\n",
    "    X_val = X[int(X.shape[0]*rate):]\n",
    "    Y_val = Y[int(Y.shape[0]*rate)-1:-1]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "df = pd.read_csv(\"data/2330_indicators.csv\")\n",
    "X_train = df.values\n",
    "y_train = df.loc[1:].values\n",
    "X_train , Y_train , X_val , Y_val = splitData(X_train,y_train,0.8)\n",
    "print(X_val,Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1             2         3         4         5  \\\n",
      "0     0.000000  0.329335  1.540300e-06  0.000000  0.000000  0.000000   \n",
      "1     0.000199  0.364776  0.000000e+00  0.000000  0.000000  0.000000   \n",
      "2     0.000398  0.402944  0.000000e+00  0.000000  0.000000  0.000000   \n",
      "3     0.000597  0.443839  5.134333e-07  0.000000  0.000000  0.000000   \n",
      "4     0.000796  0.487459  0.000000e+00  0.000000  0.000000  0.000000   \n",
      "5     0.000994  0.533806  1.849181e-05  0.534003  0.000000  0.000000   \n",
      "6     0.001193  0.582879  1.978772e-03  0.578666  0.000000  0.000000   \n",
      "7     0.001392  0.634678  6.438724e-03  0.623285  0.000000  0.000000   \n",
      "8     0.001591  0.689204  5.288582e-03  0.668657  0.000000  0.000000   \n",
      "9     0.001790  0.749182  9.123507e-03  0.712968  0.000000  0.000000   \n",
      "10    0.001989  0.738277  5.759766e-03  0.743681  0.000000  0.000000   \n",
      "11    0.002188  0.738277  4.178618e-03  0.763765  0.659673  0.000000   \n",
      "12    0.002387  0.749182  3.582738e-03  0.777936  0.686906  0.000000   \n",
      "13    0.002586  0.727372  1.745759e-03  0.783867  0.707806  0.000000   \n",
      "14    0.002784  0.760087  5.642184e-03  0.793681  0.728213  0.000000   \n",
      "15    0.002983  0.738277  3.029578e-03  0.797880  0.744023  0.000000   \n",
      "16    0.003182  0.732824  2.231381e-03  0.798335  0.756001  0.000000   \n",
      "17    0.003381  0.738277  1.891469e-03  0.799811  0.766581  0.000000   \n",
      "18    0.003580  0.743730  1.358889e-03  0.801185  0.775598  0.000000   \n",
      "19    0.003779  0.738277  1.104915e-03  0.802492  0.783311  0.000000   \n",
      "20    0.003978  0.743730  8.520457e-04  0.803363  0.789738  0.000000   \n",
      "21    0.004177  0.743730  8.004425e-04  0.804335  0.795293  0.000000   \n",
      "22    0.004375  0.754635  2.094597e-03  0.806936  0.800920  0.000000   \n",
      "23    0.004574  0.770992  4.751021e-03  0.812576  0.807603  0.000000   \n",
      "24    0.004773  0.716467  4.961760e-03  0.806179  0.807988  0.000000   \n",
      "25    0.004972  0.727372  3.598048e-03  0.801134  0.807909  0.775000   \n",
      "26    0.005171  0.667394  4.584274e-03  0.786051  0.801863  0.776987   \n",
      "27    0.005370  0.678299  4.801115e-03  0.771699  0.794631  0.777771   \n",
      "28    0.005569  0.721919  3.527075e-03  0.775803  0.795582  0.781837   \n",
      "29    0.005768  0.721919  2.464045e-03  0.778930  0.796575  0.785686   \n",
      "...        ...       ...           ...       ...       ...       ...   \n",
      "4999  0.994232  0.372955  1.021921e-02  0.489570  0.505190  0.527359   \n",
      "5000  0.994431  0.372955  1.298805e-02  0.489276  0.504136  0.526619   \n",
      "5001  0.994630  0.378408  1.771813e-02  0.490057  0.503756  0.526176   \n",
      "5002  0.994829  0.367503  2.396417e-02  0.487843  0.502044  0.525098   \n",
      "5003  0.995028  0.356598  2.926654e-02  0.483632  0.499222  0.523434   \n",
      "5004  0.995227  0.351145  2.934636e-02  0.479263  0.496072  0.521516   \n",
      "5005  0.995426  0.356598  1.588994e-02  0.477912  0.494245  0.520128   \n",
      "5006  0.995625  0.370229  2.423081e-02  0.480137  0.494318  0.519611   \n",
      "5007  0.995823  0.378408  1.173327e-02  0.484160  0.495674  0.519754   \n",
      "5008  0.996022  0.378408  1.180639e-02  0.486841  0.496804  0.519887   \n",
      "5009  0.996221  0.378408  1.769512e-02  0.488238  0.497547  0.519913   \n",
      "5010  0.996420  0.375682  9.776569e-03  0.488974  0.498066  0.519890   \n",
      "5011  0.996619  0.372955  1.376526e-02  0.488684  0.498100  0.519678   \n",
      "5012  0.996818  0.378408  1.213218e-02  0.489467  0.498627  0.519720   \n",
      "5013  0.997017  0.372955  1.294279e-02  0.489012  0.498567  0.519521   \n",
      "5014  0.997216  0.367503  1.301958e-02  0.487537  0.497919  0.519050   \n",
      "5015  0.997414  0.378408  1.230728e-02  0.488702  0.498476  0.519141   \n",
      "5016  0.997613  0.372955  1.177550e-02  0.488698  0.498541  0.519034   \n",
      "5017  0.997812  0.378408  1.263728e-02  0.489281  0.498895  0.519078   \n",
      "5018  0.998011  0.367503  9.516876e-03  0.487716  0.498192  0.518642   \n",
      "5019  0.998210  0.364776  1.452814e-02  0.485696  0.497108  0.518000   \n",
      "5020  0.998409  0.362050  9.637697e-03  0.483569  0.495807  0.517217   \n",
      "5021  0.998608  0.370229  1.346830e-02  0.484299  0.495818  0.517019   \n",
      "5022  0.998807  0.356598  3.393850e-02  0.481465  0.494133  0.516025   \n",
      "5023  0.999006  0.364776  1.428565e-02  0.480943  0.493427  0.515441   \n",
      "5024  0.999204  0.362050  1.931417e-02  0.480400  0.492738  0.514854   \n",
      "5025  0.999403  0.372955  1.457773e-02  0.482577  0.493461  0.514934   \n",
      "5026  0.999602  0.367503  7.228211e-03  0.483051  0.493564  0.514768   \n",
      "5027  0.999801  0.367503  5.025520e-03  0.482977  0.493451  0.514520   \n",
      "5028  1.000000  0.372955  3.487707e-03  0.484100  0.493955  0.514577   \n",
      "\n",
      "             6         7         8         9  \n",
      "0     0.628022  0.000000  0.000000  0.000000  \n",
      "1     0.628022  0.000000  0.000000  0.000000  \n",
      "2     0.628022  0.000000  0.000000  0.000000  \n",
      "3     0.628022  0.000000  0.000000  0.000000  \n",
      "4     0.628022  0.000000  0.000000  0.000000  \n",
      "5     0.628022  0.000000  0.000000  0.000000  \n",
      "6     0.628022  0.000000  0.000000  0.000000  \n",
      "7     0.628022  1.000000  0.511880  0.524688  \n",
      "8     0.628022  1.000000  0.682507  0.582986  \n",
      "9     0.628022  1.000000  0.796258  0.660718  \n",
      "10    0.628022  0.956553  0.841759  0.728085  \n",
      "11    0.628022  0.956553  0.868301  0.782065  \n",
      "12    0.628022  0.959112  0.893871  0.826788  \n",
      "13    0.628022  0.840360  0.874408  0.849954  \n",
      "14    0.628022  0.869454  0.904410  0.875649  \n",
      "15    0.628022  0.758819  0.862292  0.878388  \n",
      "16    0.628022  0.730917  0.756864  0.844192  \n",
      "17    0.628022  0.742288  0.675203  0.793493  \n",
      "18    0.628022  0.754727  0.601803  0.734616  \n",
      "19    0.628022  0.713408  0.529172  0.670548  \n",
      "20    0.628022  0.731075  0.523408  0.625867  \n",
      "21    0.628022  0.731075  0.519566  0.594766  \n",
      "22    0.628022  0.771621  0.602317  0.602306  \n",
      "23    0.628022  0.820371  0.742799  0.655332  \n",
      "24    0.628022  0.442527  0.495199  0.606084  \n",
      "25    0.628022  0.498015  0.398383  0.540173  \n",
      "26    0.628022  0.300564  0.265589  0.450860  \n",
      "27    0.628022  0.356251  0.266082  0.391486  \n",
      "28    0.628022  0.534244  0.385108  0.392572  \n",
      "29    0.628022  0.534244  0.464458  0.420408  \n",
      "...        ...       ...       ...       ...  \n",
      "4999  0.633886  0.338369  0.117059  0.148098  \n",
      "5000  0.628144  0.338369  0.153873  0.151306  \n",
      "5001  0.623617  0.477001  0.254250  0.187741  \n",
      "5002  0.619057  0.317393  0.212157  0.197649  \n",
      "5003  0.613710  0.226461  0.172461  0.190692  \n",
      "5004  0.607617  0.193244  0.114974  0.166411  \n",
      "5005  0.602061  0.313994  0.138695  0.158329  \n",
      "5006  0.598411  0.526572  0.309625  0.211344  \n",
      "5007  0.597190  0.612932  0.516647  0.317420  \n",
      "5008  0.597611  0.612932  0.654662  0.435294  \n",
      "5009  0.598946  0.612932  0.746672  0.545314  \n",
      "5010  0.600770  0.554654  0.776989  0.629019  \n",
      "5011  0.602561  0.497851  0.766177  0.681128  \n",
      "5012  0.604671  0.596923  0.821015  0.734604  \n",
      "5013  0.606543  0.482653  0.774846  0.754480  \n",
      "5014  0.607770  0.392491  0.644534  0.723207  \n",
      "5015  0.609405  0.580546  0.702692  0.722229  \n",
      "5016  0.610949  0.489610  0.604963  0.688186  \n",
      "5017  0.612616  0.570367  0.676312  0.689868  \n",
      "5018  0.613557  0.413387  0.450874  0.613964  \n",
      "5019  0.613661  0.381858  0.300583  0.512010  \n",
      "5020  0.612982  0.349839  0.200389  0.409808  \n",
      "5021  0.612721  0.500591  0.304219  0.377149  \n",
      "5022  0.611498  0.341995  0.202813  0.320728  \n",
      "5023  0.610320  0.464213  0.263179  0.303740  \n",
      "5024  0.609205  0.432109  0.260766  0.291590  \n",
      "5025  0.609214  0.573641  0.429784  0.341239  \n",
      "5026  0.609587  0.499020  0.481525  0.392017  \n",
      "5027  0.610062  0.499020  0.516019  0.437654  \n",
      "5028  0.611067  0.578058  0.636516  0.509250  \n",
      "\n",
      "[5029 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "sc_X = MinMaxScaler()\n",
    "X_train_sc = sc_X.fit_transform(X_train)\n",
    "X_val_sc = sc_X.transform(X_val)\n",
    "\n",
    "X_train_df = pd.DataFrame(data=X_train_sc)\n",
    "X_val_df = pd.DataFrame(data=X_val_sc)\n",
    "print(X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer#1 Volume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.32933479]]\n",
      "\n",
      " [[0.36477644]]\n",
      "\n",
      " [[0.40294438]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.36205016]]\n",
      "\n",
      " [[0.37295529]]\n",
      "\n",
      " [[0.36750273]]]\n"
     ]
    }
   ],
   "source": [
    "Volume_X_train = X_train_df.loc[:,1].values\n",
    "Volume_Y_train = Volume_X_train[1:]\n",
    "Volume_X_train = Volume_X_train[:-1]\n",
    "Volume_X_val = X_val_df.loc[:,1].values\n",
    "Volume_Y_val = Volume_X_val[1:]\n",
    "Volume_units = 128\n",
    "Volume_X_train = Volume_X_train.reshape(Volume_X_train.shape[0],1,1)\n",
    "Volume_X_val = Volume_X_val.reshape(Volume_X_val.shape[0],1,1)\n",
    "VolumeInput = Input(shape=(1,1))\n",
    "Volume_model = LSTM(Volume_units,activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True)(VolumeInput)\n",
    "Volume_out = LSTM(Volume_units,kernel_regularizer=regularizers.l2(0.001))(Volume_model)\n",
    "Volume_model = Dense(1)(Volume_out)\n",
    "#Volume_model = Model(inputs=VolumeInput,outputs=Volume_model)\n",
    "#Volume_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "#early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "#history = Volume_model.fit(Volume_X_train[:-1], Volume_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(Volume_X_val[:-1],Volume_Y_val),callbacks=[early_stop])\n",
    "#volume_y_pred = Volume_model.predict(Volume_X_val[:-1])\n",
    "#r2score = r2_score(Volume_Y_val,volume_y_pred)\n",
    "#print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))\n",
    "#plt.plot(volume_y_pred[:200])\n",
    "#plt.plot(Volume_Y_val[:200])\n",
    "#plt.show()\n",
    "print(Volume_X_train[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer#2 ema6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 1, 128)       66560       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 1, 512)       1312768     lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 1, 128)       66560       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 1, 512)       2099200     lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          131584      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 512)          2099200     lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            513         lstm_18[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,776,514\n",
      "Trainable params: 5,776,514\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "5028/5028 [==============================] - 8s 2ms/step - loss: 2.7503 - dense_1_loss: 0.0096 - dense_5_loss: 1.4173\n",
      "Epoch 2/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 2.0367 - dense_1_loss: 0.0103 - dense_5_loss: 1.3183\n",
      "Epoch 3/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.8476 - dense_1_loss: 0.0093 - dense_5_loss: 1.2549\n",
      "Epoch 4/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.7189 - dense_1_loss: 0.0090 - dense_5_loss: 1.1923\n",
      "Epoch 5/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.6508 - dense_1_loss: 0.0104 - dense_5_loss: 1.1721\n",
      "Epoch 6/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.5886 - dense_1_loss: 0.0086 - dense_5_loss: 1.1491\n",
      "Epoch 7/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.5372 - dense_1_loss: 0.0105 - dense_5_loss: 1.1270\n",
      "Epoch 8/1000\n",
      "5028/5028 [==============================] - 1s 219us/step - loss: 1.4973 - dense_1_loss: 0.0102 - dense_5_loss: 1.1143\n",
      "Epoch 9/1000\n",
      "5028/5028 [==============================] - 1s 221us/step - loss: 1.4676 - dense_1_loss: 0.0092 - dense_5_loss: 1.1089\n",
      "Epoch 10/1000\n",
      "5028/5028 [==============================] - 1s 219us/step - loss: 1.4413 - dense_1_loss: 0.0101 - dense_5_loss: 1.1026\n",
      "Epoch 11/1000\n",
      "5028/5028 [==============================] - 1s 218us/step - loss: 1.4202 - dense_1_loss: 0.0091 - dense_5_loss: 1.1015\n",
      "Epoch 12/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.4001 - dense_1_loss: 0.0101 - dense_5_loss: 1.0974\n",
      "Epoch 13/1000\n",
      "5028/5028 [==============================] - 1s 228us/step - loss: 1.3866 - dense_1_loss: 0.0099 - dense_5_loss: 1.0996\n",
      "Epoch 14/1000\n",
      "5028/5028 [==============================] - 1s 245us/step - loss: 1.3679 - dense_1_loss: 0.0090 - dense_5_loss: 1.0958\n",
      "Epoch 15/1000\n",
      "5028/5028 [==============================] - 1s 226us/step - loss: 1.3598 - dense_1_loss: 0.0098 - dense_5_loss: 1.0997\n",
      "Epoch 16/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.3433 - dense_1_loss: 0.0095 - dense_5_loss: 1.0954\n",
      "Epoch 17/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.3282 - dense_1_loss: 0.0094 - dense_5_loss: 1.0921\n",
      "Epoch 18/1000\n",
      "5028/5028 [==============================] - 1s 218us/step - loss: 1.3149 - dense_1_loss: 0.0099 - dense_5_loss: 1.0894\n",
      "Epoch 19/1000\n",
      "5028/5028 [==============================] - 1s 218us/step - loss: 1.3045 - dense_1_loss: 0.0092 - dense_5_loss: 1.0900\n",
      "Epoch 20/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.2914 - dense_1_loss: 0.0094 - dense_5_loss: 1.0861\n",
      "Epoch 21/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.2834 - dense_1_loss: 0.0099 - dense_5_loss: 1.0864\n",
      "Epoch 22/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.2707 - dense_1_loss: 0.0094 - dense_5_loss: 1.0826\n",
      "Epoch 23/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.2603 - dense_1_loss: 0.0091 - dense_5_loss: 1.0808\n",
      "Epoch 24/1000\n",
      "5028/5028 [==============================] - 1s 220us/step - loss: 1.2527 - dense_1_loss: 0.0100 - dense_5_loss: 1.0798\n",
      "Epoch 25/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.2411 - dense_1_loss: 0.0087 - dense_5_loss: 1.0766\n",
      "Epoch 26/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.2388 - dense_1_loss: 0.0102 - dense_5_loss: 1.0789\n",
      "Epoch 27/1000\n",
      "5028/5028 [==============================] - 1s 208us/step - loss: 1.2305 - dense_1_loss: 0.0094 - dense_5_loss: 1.0771\n",
      "Epoch 28/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.2238 - dense_1_loss: 0.0091 - dense_5_loss: 1.0763\n",
      "Epoch 29/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.2194 - dense_1_loss: 0.0102 - dense_5_loss: 1.0759\n",
      "Epoch 30/1000\n",
      "5028/5028 [==============================] - 1s 227us/step - loss: 1.2143 - dense_1_loss: 0.0090 - dense_5_loss: 1.0766\n",
      "Epoch 31/1000\n",
      "5028/5028 [==============================] - 1s 229us/step - loss: 1.2112 - dense_1_loss: 0.0101 - dense_5_loss: 1.0764\n",
      "Epoch 32/1000\n",
      "5028/5028 [==============================] - 1s 217us/step - loss: 1.2030 - dense_1_loss: 0.0088 - dense_5_loss: 1.0742\n",
      "Epoch 33/1000\n",
      "5028/5028 [==============================] - 1s 221us/step - loss: 1.1978 - dense_1_loss: 0.0101 - dense_5_loss: 1.0723\n",
      "Epoch 34/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1921 - dense_1_loss: 0.0096 - dense_5_loss: 1.0707\n",
      "Epoch 35/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1887 - dense_1_loss: 0.0091 - dense_5_loss: 1.0707 0s - loss: 1.0983 - dense_1_loss: 0.0106 - dense_5_loss: \n",
      "Epoch 36/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1874 - dense_1_loss: 0.0099 - dense_5_loss: 1.0718\n",
      "Epoch 37/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1804 - dense_1_loss: 0.0091 - dense_5_loss: 1.0687\n",
      "Epoch 38/1000\n",
      "5028/5028 [==============================] - 1s 210us/step - loss: 1.1797 - dense_1_loss: 0.0100 - dense_5_loss: 1.0701\n",
      "Epoch 39/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1789 - dense_1_loss: 0.0091 - dense_5_loss: 1.0723\n",
      "Epoch 40/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1780 - dense_1_loss: 0.0103 - dense_5_loss: 1.0723\n",
      "Epoch 41/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1711 - dense_1_loss: 0.0087 - dense_5_loss: 1.0697\n",
      "Epoch 42/1000\n",
      "5028/5028 [==============================] - 1s 218us/step - loss: 1.1664 - dense_1_loss: 0.0100 - dense_5_loss: 1.0664\n",
      "Epoch 43/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.1646 - dense_1_loss: 0.0094 - dense_5_loss: 1.0671\n",
      "Epoch 44/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1628 - dense_1_loss: 0.0094 - dense_5_loss: 1.0668\n",
      "Epoch 45/1000\n",
      "5028/5028 [==============================] - 1s 221us/step - loss: 1.1627 - dense_1_loss: 0.0097 - dense_5_loss: 1.0676\n",
      "Epoch 46/1000\n",
      "5028/5028 [==============================] - 1s 211us/step - loss: 1.1590 - dense_1_loss: 0.0091 - dense_5_loss: 1.0664\n",
      "Epoch 47/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1554 - dense_1_loss: 0.0099 - dense_5_loss: 1.0641\n",
      "Epoch 48/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1550 - dense_1_loss: 0.0095 - dense_5_loss: 1.0654\n",
      "Epoch 49/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1548 - dense_1_loss: 0.0097 - dense_5_loss: 1.0660\n",
      "Epoch 50/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1542 - dense_1_loss: 0.0093 - dense_5_loss: 1.0667\n",
      "Epoch 51/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1505 - dense_1_loss: 0.0096 - dense_5_loss: 1.0643\n",
      "Epoch 52/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1489 - dense_1_loss: 0.0091 - dense_5_loss: 1.0646\n",
      "Epoch 53/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1478 - dense_1_loss: 0.0099 - dense_5_loss: 1.0639\n",
      "Epoch 54/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1456 - dense_1_loss: 0.0093 - dense_5_loss: 1.0632\n",
      "Epoch 55/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.1449 - dense_1_loss: 0.0096 - dense_5_loss: 1.0629\n",
      "Epoch 56/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1437 - dense_1_loss: 0.0091 - dense_5_loss: 1.0627\n",
      "Epoch 57/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1466 - dense_1_loss: 0.0099 - dense_5_loss: 1.0650\n",
      "Epoch 58/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1442 - dense_1_loss: 0.0092 - dense_5_loss: 1.0644\n",
      "Epoch 59/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1425 - dense_1_loss: 0.0098 - dense_5_loss: 1.0630\n",
      "Epoch 60/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1404 - dense_1_loss: 0.0092 - dense_5_loss: 1.0623\n",
      "Epoch 61/1000\n",
      "5028/5028 [==============================] - 1s 211us/step - loss: 1.1395 - dense_1_loss: 0.0096 - dense_5_loss: 1.0618\n",
      "Epoch 62/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1386 - dense_1_loss: 0.0092 - dense_5_loss: 1.0621 0s - loss: 0.3070 - dense_1_loss: 0.0115 - den\n",
      "Epoch 63/1000\n",
      "5028/5028 [==============================] - 1s 214us/step - loss: 1.1378 - dense_1_loss: 0.0098 - dense_5_loss: 1.0614\n",
      "Epoch 64/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1362 - dense_1_loss: 0.0092 - dense_5_loss: 1.0610\n",
      "Epoch 65/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1357 - dense_1_loss: 0.0096 - dense_5_loss: 1.0607\n",
      "Epoch 66/1000\n",
      "5028/5028 [==============================] - 1s 211us/step - loss: 1.1345 - dense_1_loss: 0.0091 - dense_5_loss: 1.0603\n",
      "Epoch 67/1000\n",
      "5028/5028 [==============================] - 1s 215us/step - loss: 1.1355 - dense_1_loss: 0.0099 - dense_5_loss: 1.0608\n",
      "Epoch 68/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1329 - dense_1_loss: 0.0092 - dense_5_loss: 1.0595\n",
      "Epoch 69/1000\n",
      "5028/5028 [==============================] - 1s 221us/step - loss: 1.1336 - dense_1_loss: 0.0092 - dense_5_loss: 1.0604\n",
      "Epoch 70/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.1312 - dense_1_loss: 0.0102 - dense_5_loss: 1.0579\n",
      "Epoch 71/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1306 - dense_1_loss: 0.0087 - dense_5_loss: 1.0593\n",
      "Epoch 72/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1288 - dense_1_loss: 0.0099 - dense_5_loss: 1.0572\n",
      "Epoch 73/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1290 - dense_1_loss: 0.0092 - dense_5_loss: 1.0583\n",
      "Epoch 74/1000\n",
      "5028/5028 [==============================] - 1s 219us/step - loss: 1.1289 - dense_1_loss: 0.0098 - dense_5_loss: 1.0581\n",
      "Epoch 75/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1286 - dense_1_loss: 0.0090 - dense_5_loss: 1.0589\n",
      "Epoch 76/1000\n",
      "5028/5028 [==============================] - 1s 216us/step - loss: 1.1289 - dense_1_loss: 0.0099 - dense_5_loss: 1.0587\n",
      "Epoch 77/1000\n",
      "5028/5028 [==============================] - 1s 223us/step - loss: 1.1285 - dense_1_loss: 0.0091 - dense_5_loss: 1.0593\n",
      "Epoch 78/1000\n",
      "5028/5028 [==============================] - 1s 224us/step - loss: 1.1259 - dense_1_loss: 0.0098 - dense_5_loss: 1.0563\n",
      "Epoch 79/1000\n",
      "5028/5028 [==============================] - 1s 241us/step - loss: 1.1277 - dense_1_loss: 0.0093 - dense_5_loss: 1.0588\n",
      "Epoch 80/1000\n",
      "5028/5028 [==============================] - 1s 219us/step - loss: 1.1251 - dense_1_loss: 0.0092 - dense_5_loss: 1.0568\n",
      "Epoch 81/1000\n",
      "5028/5028 [==============================] - 1s 218us/step - loss: 1.1281 - dense_1_loss: 0.0099 - dense_5_loss: 1.0590\n",
      "Epoch 82/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1239 - dense_1_loss: 0.0087 - dense_5_loss: 1.0565\n",
      "Epoch 83/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1274 - dense_1_loss: 0.0097 - dense_5_loss: 1.0591\n",
      "Epoch 84/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1254 - dense_1_loss: 0.0100 - dense_5_loss: 1.0576\n",
      "Epoch 85/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1245 - dense_1_loss: 0.0088 - dense_5_loss: 1.0580\n",
      "Epoch 86/1000\n",
      "5028/5028 [==============================] - 1s 213us/step - loss: 1.1241 - dense_1_loss: 0.0099 - dense_5_loss: 1.0567\n",
      "Epoch 87/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.1239 - dense_1_loss: 0.0091 - dense_5_loss: 1.0577\n",
      "Epoch 88/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1233 - dense_1_loss: 0.0099 - dense_5_loss: 1.0566\n",
      "Epoch 89/1000\n",
      "5028/5028 [==============================] - 1s 212us/step - loss: 1.1245 - dense_1_loss: 0.0096 - dense_5_loss: 1.0581\n",
      "Epoch 90/1000\n",
      "5028/5028 [==============================] - 1s 210us/step - loss: 1.1222 - dense_1_loss: 0.0092 - dense_5_loss: 1.0567\n",
      "Epoch 91/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1237 - dense_1_loss: 0.0100 - dense_5_loss: 1.0575\n",
      "Epoch 92/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1222 - dense_1_loss: 0.0092 - dense_5_loss: 1.0571\n",
      "Epoch 93/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1209 - dense_1_loss: 0.0092 - dense_5_loss: 1.0562\n",
      "Epoch 94/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1235 - dense_1_loss: 0.0095 - dense_5_loss: 1.0587\n",
      "Epoch 95/1000\n",
      "5028/5028 [==============================] - 1s 209us/step - loss: 1.1203 - dense_1_loss: 0.0093 - dense_5_loss: 1.0560\n",
      "Epoch 96/1000\n",
      "5028/5028 [==============================] - 1s 208us/step - loss: 1.1227 - dense_1_loss: 0.0102 - dense_5_loss: 1.0575\n",
      "Epoch 97/1000\n",
      "5028/5028 [==============================] - 1s 207us/step - loss: 1.1194 - dense_1_loss: 0.0085 - dense_5_loss: 1.0564\n",
      "Epoch 98/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1224 - dense_1_loss: 0.0094 - dense_5_loss: 1.0584\n",
      "Epoch 99/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1214 - dense_1_loss: 0.0101 - dense_5_loss: 1.0571\n",
      "Epoch 100/1000\n",
      "5028/5028 [==============================] - 1s 211us/step - loss: 1.1211 - dense_1_loss: 0.0086 - dense_5_loss: 1.0582\n",
      "Epoch 101/1000\n",
      "5028/5028 [==============================] - 1s 223us/step - loss: 1.1203 - dense_1_loss: 0.0099 - dense_5_loss: 1.0566\n",
      "Epoch 102/1000\n",
      "5028/5028 [==============================] - 1s 210us/step - loss: 1.1203 - dense_1_loss: 0.0096 - dense_5_loss: 1.0571\n",
      "Epoch 103/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1197 - dense_1_loss: 0.0096 - dense_5_loss: 1.0568\n",
      "Epoch 104/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1194 - dense_1_loss: 0.0092 - dense_5_loss: 1.0571\n",
      "Epoch 105/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1171 - dense_1_loss: 0.0094 - dense_5_loss: 1.0555\n",
      "Epoch 106/1000\n",
      "5028/5028 [==============================] - 1s 197us/step - loss: 1.1168 - dense_1_loss: 0.0094 - dense_5_loss: 1.0554\n",
      "Epoch 107/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1161 - dense_1_loss: 0.0098 - dense_5_loss: 1.0545\n",
      "Epoch 108/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1157 - dense_1_loss: 0.0091 - dense_5_loss: 1.0549\n",
      "Epoch 109/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1159 - dense_1_loss: 0.0094 - dense_5_loss: 1.0550\n",
      "Epoch 110/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1171 - dense_1_loss: 0.0099 - dense_5_loss: 1.0556\n",
      "Epoch 111/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1148 - dense_1_loss: 0.0091 - dense_5_loss: 1.0548\n",
      "Epoch 112/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1161 - dense_1_loss: 0.0100 - dense_5_loss: 1.0552\n",
      "Epoch 113/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1148 - dense_1_loss: 0.0087 - dense_5_loss: 1.0552\n",
      "Epoch 114/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1154 - dense_1_loss: 0.0096 - dense_5_loss: 1.0551\n",
      "Epoch 115/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1155 - dense_1_loss: 0.0099 - dense_5_loss: 1.0551\n",
      "Epoch 116/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1151 - dense_1_loss: 0.0092 - dense_5_loss: 1.0554\n",
      "Epoch 117/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1148 - dense_1_loss: 0.0092 - dense_5_loss: 1.0553\n",
      "Epoch 118/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1151 - dense_1_loss: 0.0093 - dense_5_loss: 1.0558\n",
      "Epoch 119/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1148 - dense_1_loss: 0.0099 - dense_5_loss: 1.0554\n",
      "Epoch 120/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1133 - dense_1_loss: 0.0093 - dense_5_loss: 1.0547\n",
      "Epoch 121/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1133 - dense_1_loss: 0.0092 - dense_5_loss: 1.0548\n",
      "Epoch 122/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1142 - dense_1_loss: 0.0099 - dense_5_loss: 1.0552\n",
      "Epoch 123/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1127 - dense_1_loss: 0.0089 - dense_5_loss: 1.0550\n",
      "Epoch 124/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1128 - dense_1_loss: 0.0099 - dense_5_loss: 1.0545\n",
      "Epoch 125/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1120 - dense_1_loss: 0.0092 - dense_5_loss: 1.0544\n",
      "Epoch 126/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1123 - dense_1_loss: 0.0094 - dense_5_loss: 1.0547\n",
      "Epoch 127/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1117 - dense_1_loss: 0.0092 - dense_5_loss: 1.0547\n",
      "Epoch 128/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1117 - dense_1_loss: 0.0094 - dense_5_loss: 1.0547\n",
      "Epoch 129/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1117 - dense_1_loss: 0.0099 - dense_5_loss: 1.0540\n",
      "Epoch 130/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1112 - dense_1_loss: 0.0089 - dense_5_loss: 1.0546\n",
      "Epoch 131/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1114 - dense_1_loss: 0.0095 - dense_5_loss: 1.0548\n",
      "Epoch 132/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1111 - dense_1_loss: 0.0100 - dense_5_loss: 1.0540\n",
      "Epoch 133/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1108 - dense_1_loss: 0.0088 - dense_5_loss: 1.0545\n",
      "Epoch 134/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1123 - dense_1_loss: 0.0100 - dense_5_loss: 1.0551\n",
      "Epoch 135/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1098 - dense_1_loss: 0.0088 - dense_5_loss: 1.0546\n",
      "Epoch 136/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1103 - dense_1_loss: 0.0099 - dense_5_loss: 1.0539\n",
      "Epoch 137/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1107 - dense_1_loss: 0.0095 - dense_5_loss: 1.0544 0s - loss: 1.1169 - dense_1_loss: 0.0100 - dense_5_loss: 1.0\n",
      "Epoch 138/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1108 - dense_1_loss: 0.0096 - dense_5_loss: 1.0547\n",
      "Epoch 139/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1096 - dense_1_loss: 0.0093 - dense_5_loss: 1.0543\n",
      "Epoch 140/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1095 - dense_1_loss: 0.0092 - dense_5_loss: 1.0543\n",
      "Epoch 141/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1105 - dense_1_loss: 0.0098 - dense_5_loss: 1.0545\n",
      "Epoch 142/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1095 - dense_1_loss: 0.0092 - dense_5_loss: 1.0544\n",
      "Epoch 143/1000\n",
      "5028/5028 [==============================] - 1s 210us/step - loss: 1.1101 - dense_1_loss: 0.0098 - dense_5_loss: 1.0546\n",
      "Epoch 144/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1093 - dense_1_loss: 0.0095 - dense_5_loss: 1.0544\n",
      "Epoch 145/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1089 - dense_1_loss: 0.0092 - dense_5_loss: 1.0546\n",
      "Epoch 146/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1098 - dense_1_loss: 0.0098 - dense_5_loss: 1.0549\n",
      "Epoch 147/1000\n",
      "5028/5028 [==============================] - 1s 198us/step - loss: 1.1087 - dense_1_loss: 0.0092 - dense_5_loss: 1.0545\n",
      "Epoch 148/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1100 - dense_1_loss: 0.0098 - dense_5_loss: 1.0552\n",
      "Epoch 149/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1093 - dense_1_loss: 0.0093 - dense_5_loss: 1.0551\n",
      "Epoch 150/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1090 - dense_1_loss: 0.0092 - dense_5_loss: 1.0552\n",
      "Epoch 151/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1085 - dense_1_loss: 0.0095 - dense_5_loss: 1.0545\n",
      "Epoch 152/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1090 - dense_1_loss: 0.0092 - dense_5_loss: 1.0555\n",
      "Epoch 153/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1070 - dense_1_loss: 0.0091 - dense_5_loss: 1.0539\n",
      "Epoch 154/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1082 - dense_1_loss: 0.0100 - dense_5_loss: 1.0543\n",
      "Epoch 155/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1076 - dense_1_loss: 0.0087 - dense_5_loss: 1.0550\n",
      "Epoch 156/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1076 - dense_1_loss: 0.0097 - dense_5_loss: 1.0544\n",
      "Epoch 157/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1078 - dense_1_loss: 0.0097 - dense_5_loss: 1.0545\n",
      "Epoch 158/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1065 - dense_1_loss: 0.0092 - dense_5_loss: 1.0539\n",
      "Epoch 159/1000\n",
      "5028/5028 [==============================] - 1s 208us/step - loss: 1.1077 - dense_1_loss: 0.0092 - dense_5_loss: 1.0551 0s - loss: 0.6950 - dense_1_loss: 0.0135 - dense\n",
      "Epoch 160/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1074 - dense_1_loss: 0.0096 - dense_5_loss: 1.0548\n",
      "Epoch 161/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1079 - dense_1_loss: 0.0090 - dense_5_loss: 1.0563\n",
      "Epoch 162/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1077 - dense_1_loss: 0.0098 - dense_5_loss: 1.0554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1080 - dense_1_loss: 0.0092 - dense_5_loss: 1.0565\n",
      "Epoch 164/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1078 - dense_1_loss: 0.0093 - dense_5_loss: 1.0565\n",
      "Epoch 165/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1085 - dense_1_loss: 0.0095 - dense_5_loss: 1.0572\n",
      "Epoch 166/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1067 - dense_1_loss: 0.0090 - dense_5_loss: 1.0563 0s - loss: 0.7129 - dense_1_loss: 0.0144 - dense_5_\n",
      "Epoch 167/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1074 - dense_1_loss: 0.0097 - dense_5_loss: 1.0566\n",
      "Epoch 168/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1063 - dense_1_loss: 0.0093 - dense_5_loss: 1.0564\n",
      "Epoch 169/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1076 - dense_1_loss: 0.0092 - dense_5_loss: 1.0580\n",
      "Epoch 170/1000\n",
      "5028/5028 [==============================] - 1s 204us/step - loss: 1.1065 - dense_1_loss: 0.0100 - dense_5_loss: 1.0564\n",
      "Epoch 171/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1079 - dense_1_loss: 0.0087 - dense_5_loss: 1.0593\n",
      "Epoch 172/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1060 - dense_1_loss: 0.0093 - dense_5_loss: 1.0570\n",
      "Epoch 173/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1088 - dense_1_loss: 0.0100 - dense_5_loss: 1.0590\n",
      "Epoch 174/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1056 - dense_1_loss: 0.0088 - dense_5_loss: 1.0568\n",
      "Epoch 175/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1083 - dense_1_loss: 0.0100 - dense_5_loss: 1.0578 0s - loss: 0.9844 - dense_1_loss: 0.0124 - dense_5_loss:\n",
      "Epoch 176/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1053 - dense_1_loss: 0.0087 - dense_5_loss: 1.0556\n",
      "Epoch 177/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1085 - dense_1_loss: 0.0100 - dense_5_loss: 1.0567\n",
      "Epoch 178/1000\n",
      "5028/5028 [==============================] - 1s 197us/step - loss: 1.1054 - dense_1_loss: 0.0086 - dense_5_loss: 1.0546\n",
      "Epoch 179/1000\n",
      "5028/5028 [==============================] - 1s 201us/step - loss: 1.1091 - dense_1_loss: 0.0099 - dense_5_loss: 1.0572\n",
      "Epoch 180/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1059 - dense_1_loss: 0.0093 - dense_5_loss: 1.0549\n",
      "Epoch 181/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1097 - dense_1_loss: 0.0097 - dense_5_loss: 1.0581\n",
      "Epoch 182/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1063 - dense_1_loss: 0.0093 - dense_5_loss: 1.0561\n",
      "Epoch 183/1000\n",
      "5028/5028 [==============================] - 1s 206us/step - loss: 1.1086 - dense_1_loss: 0.0096 - dense_5_loss: 1.0591\n",
      "Epoch 184/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1037 - dense_1_loss: 0.0085 - dense_5_loss: 1.0566\n",
      "Epoch 185/1000\n",
      "5028/5028 [==============================] - 1s 202us/step - loss: 1.1108 - dense_1_loss: 0.0102 - dense_5_loss: 1.0622\n",
      "Epoch 186/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1054 - dense_1_loss: 0.0089 - dense_5_loss: 1.0585\n",
      "Epoch 187/1000\n",
      "5028/5028 [==============================] - 1s 200us/step - loss: 1.1114 - dense_1_loss: 0.0100 - dense_5_loss: 1.0628\n",
      "Epoch 188/1000\n",
      "5028/5028 [==============================] - 1s 199us/step - loss: 1.1030 - dense_1_loss: 0.0086 - dense_5_loss: 1.0555\n",
      "Epoch 189/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1076 - dense_1_loss: 0.0096 - dense_5_loss: 1.0588\n",
      "Epoch 190/1000\n",
      "5028/5028 [==============================] - 1s 205us/step - loss: 1.1042 - dense_1_loss: 0.0100 - dense_5_loss: 1.0551\n",
      "Epoch 191/1000\n",
      "5028/5028 [==============================] - 1s 203us/step - loss: 1.1070 - dense_1_loss: 0.0088 - dense_5_loss: 1.0591\n",
      "Epoch 192/1000\n",
      "5028/5028 [==============================] - 1s 209us/step - loss: 1.1038 - dense_1_loss: 0.0100 - dense_5_loss: 1.0551\n",
      "Epoch 193/1000\n",
      "5028/5028 [==============================] - 1s 241us/step - loss: 1.1062 - dense_1_loss: 0.0086 - dense_5_loss: 1.0593\n",
      "Epoch 194/1000\n",
      "5028/5028 [==============================] - 1s 235us/step - loss: 1.1014 - dense_1_loss: 0.0086 - dense_5_loss: 1.0551\n",
      "Epoch 195/1000\n",
      "5028/5028 [==============================] - 1s 236us/step - loss: 1.1091 - dense_1_loss: 0.0101 - dense_5_loss: 1.0613\n",
      "Epoch 196/1000\n",
      "5028/5028 [==============================] - 1s 244us/step - loss: 1.1026 - dense_1_loss: 0.0089 - dense_5_loss: 1.0564\n",
      "Epoch 197/1000\n",
      "5028/5028 [==============================] - 1s 238us/step - loss: 1.1082 - dense_1_loss: 0.0100 - dense_5_loss: 1.0606\n",
      "Epoch 198/1000\n",
      "5028/5028 [==============================] - 1s 239us/step - loss: 1.1037 - dense_1_loss: 0.0092 - dense_5_loss: 1.0565\n",
      "Epoch 199/1000\n",
      "5028/5028 [==============================] - 1s 235us/step - loss: 1.1057 - dense_1_loss: 0.0100 - dense_5_loss: 1.0576\n",
      "Epoch 200/1000\n",
      "5028/5028 [==============================] - 1s 250us/step - loss: 1.1028 - dense_1_loss: 0.0089 - dense_5_loss: 1.0564\n",
      "Epoch 201/1000\n",
      "5028/5028 [==============================] - 1s 270us/step - loss: 1.1071 - dense_1_loss: 0.0101 - dense_5_loss: 1.0597\n",
      "Epoch 202/1000\n",
      "5028/5028 [==============================] - 1s 265us/step - loss: 1.1032 - dense_1_loss: 0.0086 - dense_5_loss: 1.0575\n",
      "Epoch 203/1000\n",
      "5028/5028 [==============================] - 1s 261us/step - loss: 1.1063 - dense_1_loss: 0.0101 - dense_5_loss: 1.0588\n",
      "Epoch 204/1000\n",
      "5028/5028 [==============================] - 1s 232us/step - loss: 1.1022 - dense_1_loss: 0.0090 - dense_5_loss: 1.0561\n",
      "Epoch 205/1000\n",
      "5028/5028 [==============================] - 1s 240us/step - loss: 1.1068 - dense_1_loss: 0.0098 - dense_5_loss: 1.0595\n",
      "Epoch 206/1000\n",
      "5028/5028 [==============================] - 1s 242us/step - loss: 1.1033 - dense_1_loss: 0.0091 - dense_5_loss: 1.0565\n",
      "Epoch 207/1000\n",
      "5028/5028 [==============================] - 1s 243us/step - loss: 1.1076 - dense_1_loss: 0.0096 - dense_5_loss: 1.0604\n",
      "Epoch 208/1000\n",
      "5028/5028 [==============================] - 1s 245us/step - loss: 1.1043 - dense_1_loss: 0.0091 - dense_5_loss: 1.0578\n",
      "Epoch 209/1000\n",
      "5028/5028 [==============================] - 1s 243us/step - loss: 1.1070 - dense_1_loss: 0.0094 - dense_5_loss: 1.0601\n",
      "Epoch 210/1000\n",
      "5028/5028 [==============================] - 1s 244us/step - loss: 1.1023 - dense_1_loss: 0.0091 - dense_5_loss: 1.0564\n",
      "Epoch 211/1000\n",
      "5028/5028 [==============================] - 1s 240us/step - loss: 1.1075 - dense_1_loss: 0.0100 - dense_5_loss: 1.0607\n",
      "Epoch 212/1000\n",
      "5028/5028 [==============================] - 1s 247us/step - loss: 1.1022 - dense_1_loss: 0.0087 - dense_5_loss: 1.0568\n",
      "Epoch 213/1000\n",
      "5028/5028 [==============================] - 1s 248us/step - loss: 1.1077 - dense_1_loss: 0.0098 - dense_5_loss: 1.0610\n",
      "Epoch 214/1000\n",
      "5028/5028 [==============================] - 1s 251us/step - loss: 1.1031 - dense_1_loss: 0.0092 - dense_5_loss: 1.0571\n",
      "Epoch 00214: early stopping\n",
      "The R2 score on the Validation set is:\t0.110\n"
     ]
    }
   ],
   "source": [
    "ema6_X_train = X_train_df.loc[:,2].values *150\n",
    "ema6_Y_train = ema6_X_train[1:] \n",
    "ema6_X_train = ema6_X_train[:-1]\n",
    "ema6_X_val = X_val_df.loc[:,2].values * 150\n",
    "ema6_Y_val = ema6_X_val[1:]\n",
    "#ema6_X_val = ema6_X_val[:-1]\n",
    "ema6_units = 512\n",
    "ema6_X_train = ema6_X_train.reshape(ema6_X_train.shape[0],1,1)\n",
    "ema6_X_val = ema6_X_val.reshape(ema6_X_val.shape[0],1,1)\n",
    "ema6Input = Input(shape=(1,1))\n",
    "ema6_model = Concatenate([Volume_out,ema6Input])\n",
    "ema6_model = LSTM(Volume_units,activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True)(ema6Input)\n",
    "ema6_model = LSTM(ema6_units,kernel_regularizer=regularizers.l2(0.001),return_sequences=True)(ema6_model)\n",
    "ema6_model = LSTM(ema6_units,kernel_regularizer=regularizers.l2(0.001),return_sequences=True)(ema6_model)\n",
    "ema6_model = LSTM(ema6_units,kernel_regularizer=regularizers.l2(0.001))(ema6_model)\n",
    "ema6_model = Dense(1)(ema6_model)\n",
    "ema6_model = Model(inputs=[VolumeInput,ema6Input],outputs=[Volume_model,ema6_model])\n",
    "ema6_model.summary()\n",
    "ema6_model.compile(loss='mean_absolute_error',optimizer=\"Adam\")\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, verbose=1)\n",
    "#print(ema6_X_train[:10],ema6_Y_train[:10])\n",
    "history = ema6_model.fit([Volume_X_train,ema6_X_train], [Volume_Y_train , ema6_Y_train], epochs=1000, batch_size=128, shuffle=False,callbacks=[early_stop])\n",
    "ema6_y_pred = ema6_model.predict([Volume_X_val[:-1],ema6_X_val[:-1]])[1]\n",
    "r2score = r2_score(ema6_Y_val,ema6_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.430018 ]\n",
      " [3.563871 ]\n",
      " [2.4588487]\n",
      " ...\n",
      " [2.8603332]\n",
      " [2.7647905]\n",
      " [2.3889322]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ema6_y_pred = ema6_model.predict([Volume_X_val[:-1],ema6_X_val[:-1]])\n",
    "print(ema6_y_pred[1])\n",
    "plt.plot(ema6_y_pred[1][:100])\n",
    "plt.plot(ema6_Y_val[:100])\n",
    "plt.show()\n",
    "#print(np.sum(np.absolute(np.subtract(np.reshape(ema6_y_pred[1:],(ema6_y_pred.shape[0]-1,)),np.reshape(ema6_Y_val[:-1],(ema6_Y_val.shape[0]-1,)))))/(len(ema6_y_pred)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer#2 ema12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema12_X_train = X_train_df.loc[:,2].values\n",
    "ema12_Y_train = ema12_X_train[1:]\n",
    "ema12_X_val = X_val_df.loc[:,2].values\n",
    "ema12_Y_val = ema12_X_val[1:]\n",
    "print(ema12_X_train)\n",
    "ema12_units = 512\n",
    "ema12_X_train = ema12_X_train.reshape(ema12_X_train.shape[0],1,1)\n",
    "ema12_X_val = ema12_X_val.reshape(ema12_X_val.shape[0],1,1)\n",
    "ema12_model = Sequential()\n",
    "ema12_model.add(LSTM(ema12_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#ema12_model.add(Dropout(0.01))\n",
    "ema12_model.add(LSTM(ema12_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#ema12_model.add(Dropout(0.01))\n",
    "ema12_model.add(Dense(1))\n",
    "ema12_model.summary()\n",
    "ema12_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = ema12_model.fit(ema12_X_train[:-1], ema12_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(ema12_X_val[:-1],ema12_Y_val),callbacks=[early_stop])\n",
    "ema12_y_pred = ema12_model.predict(ema12_X_val[:-1])\n",
    "r2score = r2_score(ema12_Y_val,ema12_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_X_train = X_train_df.loc[:,2].values\n",
    "macd_Y_train = macd_X_train[1:]\n",
    "macd_X_val = X_val_df.loc[:,2].values\n",
    "macd_Y_val = macd_X_val[1:]\n",
    "print(macd_X_train)\n",
    "macd_units = 512\n",
    "macd_X_train = macd_X_train.reshape(macd_X_train.shape[0],1,1)\n",
    "macd_X_val = macd_X_val.reshape(macd_X_val.shape[0],1,1)\n",
    "macd_model = Sequential()\n",
    "macd_model.add(LSTM(macd_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#macd_model.add(Dropout(0.01))\n",
    "macd_model.add(LSTM(macd_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#macd_model.add(Dropout(0.01))\n",
    "macd_model.add(Dense(1))\n",
    "macd_model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "macd_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = macd_model.fit(macd_X_train[:-1], macd_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(macd_X_val[:-1],macd_Y_val),callbacks=[early_stop])\n",
    "macd_y_pred = macd_model.predict(macd_X_val[:-1])\n",
    "r2score = r2_score(macd_Y_val,macd_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema26_X_train = X_train_df.loc[:,2].values\n",
    "ema26_Y_train = ema26_X_train[1:]\n",
    "ema26_X_val = X_val_df.loc[:,2].values\n",
    "ema26_Y_val = ema26_X_val[1:]\n",
    "print(ema26_X_train)\n",
    "ema26_units = 512\n",
    "ema26_X_train = ema26_X_train.reshape(ema26_X_train.shape[0],1,1)\n",
    "ema26_X_val = ema26_X_val.reshape(ema26_X_val.shape[0],1,1)\n",
    "ema26_model = Sequential()\n",
    "ema26_model.add(LSTM(ema26_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#ema26_model.add(Dropout(0.01))\n",
    "ema26_model.add(LSTM(ema26_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#ema26_model.add(Dropout(0.01))\n",
    "ema26_model.add(Dense(1))\n",
    "ema26_model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "ema26_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = ema26_model.fit(ema26_X_train[:-1], ema26_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(ema26_X_val[:-1],ema26_Y_val),callbacks=[early_stop])\n",
    "ema26_y_pred = ema26_model.predict(ema26_X_val[:-1])\n",
    "r2score = r2_score(ema26_Y_val,ema26_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrsi6_X_train = X_train_df.loc[:,2].values\n",
    "wrsi6_Y_train = wrsi6_X_train[1:]\n",
    "wrsi6_X_val = X_val_df.loc[:,2].values\n",
    "wrsi6_Y_val = wrsi6_X_val[1:]\n",
    "print(wrsi6_X_train)\n",
    "wrsi6_units = 512\n",
    "wrsi6_X_train = wrsi6_X_train.reshape(wrsi6_X_train.shape[0],1,1)\n",
    "wrsi6_X_val = wrsi6_X_val.reshape(wrsi6_X_val.shape[0],1,1)\n",
    "wrsi6_model = Sequential()\n",
    "wrsi6_model.add(LSTM(wrsi6_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#wrsi6_model.add(Dropout(0.01))\n",
    "wrsi6_model.add(LSTM(wrsi6_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#wrsi6_model.add(Dropout(0.01))\n",
    "wrsi6_model.add(Dense(1))\n",
    "wrsi6_model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "wrsi6_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = wrsi6_model.fit(wrsi6_X_train[:-1], wrsi6_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(wrsi6_X_val[:-1],wrsi6_Y_val),callbacks=[early_stop])\n",
    "wrsi6_y_pred = wrsi6_model.predict(wrsi6_X_val[:-1])\n",
    "r2score = r2_score(wrsi6_Y_val,wrsi6_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k9_X_train = X_train_df.loc[:,2].values\n",
    "k9_Y_train = k9_X_train[1:]\n",
    "k9_X_val = X_val_df.loc[:,2].values\n",
    "k9_Y_val = k9_X_val[1:]\n",
    "print(k9_X_train)\n",
    "k9_units = 512\n",
    "k9_X_train = k9_X_train.reshape(k9_X_train.shape[0],1,1)\n",
    "k9_X_val = k9_X_val.reshape(k9_X_val.shape[0],1,1)\n",
    "k9_model = Sequential()\n",
    "k9_model.add(LSTM(k9_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#k9_model.add(Dropout(0.01))\n",
    "k9_model.add(LSTM(k9_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#k9_model.add(Dropout(0.01))\n",
    "k9_model.add(Dense(1))\n",
    "k9_model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "k9_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = k9_model.fit(k9_X_train[:-1], k9_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(k9_X_val[:-1],k9_Y_val),callbacks=[early_stop])\n",
    "k9_y_pred = k9_model.predict(k9_X_val[:-1])\n",
    "r2score = r2_score(k9_Y_val,k9_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d9_X_train = X_train_df.loc[:,2].values\n",
    "d9_Y_train = d9_X_train[1:]\n",
    "d9_X_val = X_val_df.loc[:,2].values\n",
    "d9_Y_val = d9_X_val[1:]\n",
    "print(d9_X_train)\n",
    "d9_units = 512\n",
    "d9_X_train = d9_X_train.reshape(d9_X_train.shape[0],1,1)\n",
    "d9_X_val = d9_X_val.reshape(d9_X_val.shape[0],1,1)\n",
    "d9_model = Sequential()\n",
    "d9_model.add(LSTM(d9_units,input_shape=(1,1),activation='relu',kernel_initializer='lecun_uniform',kernel_regularizer=regularizers.l2(0.001),return_sequences=True))\n",
    "#d9_model.add(Dropout(0.01))\n",
    "d9_model.add(LSTM(d9_units,kernel_regularizer=regularizers.l2(0.001)))\n",
    "#d9_model.add(Dropout(0.01))\n",
    "d9_model.add(Dense(1))\n",
    "d9_model.summary()\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "d9_model.compile(loss='mean_squared_error',optimizer=\"Adam\")\n",
    "history = d9_model.fit(d9_X_train[:-1], d9_Y_train, epochs=200, batch_size=32, shuffle=False,validation_data=(d9_X_val[:-1],d9_Y_val),callbacks=[early_stop])\n",
    "d9_y_pred = d9_model.predict(d9_X_val[:-1])\n",
    "r2score = r2_score(d9_Y_val,d9_y_pred)\n",
    "print(\"The R2 score on the Validation set is:\\t{:0.3f}\".format(r2score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
